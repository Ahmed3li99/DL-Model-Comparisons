{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "authorship_tag": "ABX9TyNw5l1GBFoX99o+arz/cLbT"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA_-fO_8egRa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size = 10000     # use only the 10,000 most common words\n",
    "maxlen = 200           # each review will be truncated or padded to 200 words\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "#Each review is already converted into a list of word IDs (integers).\n",
    "#Example: [1, 14, 22, 16, 43, ...] ‚Üí represents a sentence.\n"
   ],
   "metadata": {
    "id": "GSTsyLpiejhJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(X_train[0])\n"
   ],
   "metadata": {
    "id": "Fhfp2mXHfJQq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
    "\n",
    "print(\"Decoded Review:\", decode_review(X_train[0]))\n",
    "print(\"Label:\", y_train[0])\n",
    "\n",
    "#IMDB encodes words as numbers. This function converts them back to words so we can see the actual review."
   ],
   "metadata": {
    "id": "bg6lRssSejez"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "#Not all reviews have the same length.\n",
    "#Padding ensures all sequences are exactly maxlen words long so they fit into one tensor."
   ],
   "metadata": {
    "id": "6-Lbp5qbe7TQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "[\"The movie was great\"] ‚Üí [the, movie, was, great, <pad.>, <pad.>, ...]\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "CAcczl0efori"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Step 1: Word Embedding\n",
    "#Embedding: Converts each word index ‚Üí 128-dim vector (learned automatically).\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))\n",
    "\n",
    "# Step 2: Recurrent Layer\n",
    "#SimpleRNN: Reads one word at a time, keeps a ‚Äúmemory‚Äù of what it has seen before.\n",
    "model.add(SimpleRNN(128, activation='tanh'))\n",
    "\n",
    "# Step 3: Output Layer\n",
    "#Dense: Outputs a single value between 0 and 1 (positive or negative sentiment).\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ],
   "metadata": {
    "id": "rVwjxahte7QV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "opt = SGD(learning_rate=0.1, momentum=0.1)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "id": "j5hVMzVie7Jb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n"
   ],
   "metadata": {
    "id": "bOO17dO7gHO4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ],
   "metadata": {
    "id": "UTYbLlaxgJuI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concept Recap\n",
    "\n",
    "| Concept | Meaning |\n",
    "|----------|----------|\n",
    "| **RNN** | A network that has memory ‚Äî it processes data sequentially (like text or time series). |\n",
    "| **Hidden state** | Internal memory that carries information from previous steps. |\n",
    "| **Limitation** | SimpleRNN struggles with long sentences due to vanishing gradients (LSTM solves that). |\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "KaDovRjrgage"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Advantages and Disadvantages of RNN\n",
    "\n",
    "###  Advantages\n",
    "- Can handle **sequential data** (like text, speech, or time series).  \n",
    "- **Remembers previous inputs** through hidden states.  \n",
    "- Works well for **language modeling** and **sequence prediction** tasks.  \n",
    "- **Same weights** used across time steps ‚Äî reduces model complexity.  \n",
    "\n",
    "###  Disadvantages\n",
    "- **Vanishing gradient problem** ‚Äî struggles with long sequences.  \n",
    "- **Training is slow** due to sequential processing.  \n",
    "- **Difficult to parallelize**, unlike CNNs or Transformers.  \n",
    "- Can **forget long-term dependencies** in long data sequences.  \n"
   ],
   "metadata": {
    "id": "nuR59n5rjqPS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Project 1 RNN (Fill missing words)**"
   ],
   "metadata": {
    "id": "Dq4Kd6rxmRIW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================\n",
    "# üß† Simple RNN Text Prediction Example\n",
    "# ==============================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Small dataset (sentences)\n",
    "sentences = [\n",
    "    \"hard work leads to success\",\n",
    "    \"practice makes you better\",\n",
    "    \"never stop learning new things\",\n",
    "    \"dream big and stay focused\",\n",
    "    \"success comes from consistency\",\n",
    "    \"believe in yourself and work hard\",\n",
    "    \"great things take time\",\n",
    "    \"failure is the first step to success\",\n",
    "    \"push yourself because no one else will do it for you\",\n",
    "    \"discipline beats motivation every single time\"\n",
    "]\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create training data (input sequences)\n",
    "input_sequences = []\n",
    "for line in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
    "\n",
    "# Split inputs (X) and labels (y)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# Convert labels to one-hot\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Build RNN model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 10, input_length=max_seq_len-1),\n",
    "    SimpleRNN(50),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=300, verbose=0)\n",
    "\n",
    "print(\"‚úÖ Model trained successfully!\")\n"
   ],
   "metadata": {
    "id": "cq4-gNvwkbgw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ==============================================================\n",
    "# üí¨ Try your own input phrase\n",
    "# ==============================================================\n",
    "\n",
    "def predict_next_word(seed_text, model, tokenizer, max_seq_len):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == np.argmax(predicted):\n",
    "            output_word = word\n",
    "            break\n",
    "    return output_word\n",
    "\n",
    "# üîπ Try your sentence here:\n",
    "seed_text = \"practice makes you\"\n",
    "next_word = predict_next_word(seed_text, model, tokenizer, max_seq_len)\n",
    "print(f\"üß† Input: '{seed_text}' ‚Üí Predicted next word: '{next_word}'\")\n"
   ],
   "metadata": {
    "id": "O-uavuNRkbeM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **project example 2 (Mini RNN Translator (English ‚Üí Spanish) ‚Äî Working Version)**"
   ],
   "metadata": {
    "id": "N1nPPJD5l4qU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ‚ú® Small dataset for demo (English ‚Üí Spanish)\n",
    "data = {\n",
    "    \"hello\": \"hola\",\n",
    "    \"how are you\": \"como estas\",\n",
    "    \"good morning\": \"buenos dias\",\n",
    "    \"good night\": \"buenas noches\",\n",
    "    \"thank you\": \"gracias\",\n",
    "    \"i love you\": \"te amo\",\n",
    "    \"see you later\": \"hasta luego\",\n",
    "    \"my name is\": \"mi nombre es\",\n",
    "    \"what is your name\": \"como te llamas\",\n",
    "    \"have a nice day\": \"que tengas un buen dia\"\n",
    "}\n",
    "\n",
    "# Prepare source (English) and target (Spanish)\n",
    "eng_texts = list(data.keys())\n",
    "spa_texts = [\"<start> \" + txt + \" <end>\" for txt in data.values()]\n",
    "\n",
    "# Tokenize both languages\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(eng_texts)\n",
    "spa_tokenizer = Tokenizer()\n",
    "spa_tokenizer.fit_on_texts(spa_texts)\n",
    "\n",
    "eng_seq = eng_tokenizer.texts_to_sequences(eng_texts)\n",
    "spa_seq = spa_tokenizer.texts_to_sequences(spa_texts)\n",
    "\n",
    "max_eng_len = max(len(seq) for seq in eng_seq)\n",
    "max_spa_len = max(len(seq) for seq in spa_seq)\n",
    "\n",
    "eng_seq = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
    "spa_seq = pad_sequences(spa_seq, maxlen=max_spa_len, padding='post')\n",
    "\n",
    "eng_vocab = len(eng_tokenizer.word_index) + 1\n",
    "spa_vocab = len(spa_tokenizer.word_index) + 1\n",
    "\n",
    "spa_input = spa_seq[:, :-1]\n",
    "spa_target = spa_seq[:, 1:]\n",
    "\n",
    "# ------------------------------\n",
    "# Build Seq2Seq Model (Encoder-Decoder)\n",
    "# ------------------------------\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_eng_len,))\n",
    "enc_emb = Embedding(eng_vocab, 64)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_spa_len - 1,))\n",
    "dec_emb_layer = Embedding(spa_vocab, 64)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(spa_vocab, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Full training model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit([eng_seq, spa_input], np.expand_dims(spa_target, -1), epochs=300, verbose=0)\n",
    "\n",
    "print(\"‚úÖ Model trained successfully!\")\n",
    "\n",
    "# ------------------------------\n",
    "# Create Inference Models\n",
    "# ------------------------------\n",
    "# Encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + dec_states_inputs, [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# ------------------------------\n",
    "# Translation Function\n",
    "# ------------------------------\n",
    "def translate(sentence):\n",
    "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    seq = pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
    "    states_value = encoder_model.predict(seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = spa_tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = ''\n",
    "        for word, index in spa_tokenizer.word_index.items():\n",
    "            if index == sampled_token_index:\n",
    "                sampled_word = word\n",
    "                break\n",
    "        if sampled_word == 'end' or len(decoded_sentence.split()) > max_spa_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ],
   "metadata": {
    "id": "yUoBfjJElQqW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Try it interactively üöÄ\n",
    "input_sentence = \"good morning\"\n",
    "print(f\"üåç English: {input_sentence}\")\n",
    "print(f\"üá™üá∏ Spanish: {translate(input_sentence)}\")\n"
   ],
   "metadata": {
    "id": "md6if2F9lQlE"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}