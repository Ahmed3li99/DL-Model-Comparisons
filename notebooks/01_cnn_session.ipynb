{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNbDtYd1kWevRAHixyYaQ0W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! pip install tensorflow"],"metadata":{"id":"o2PJdANhBDx9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# üéØ INTRODUCTION ‚Äî Deep Learning with Images\n","# ==========================================================\n","# In this notebook, we‚Äôll understand how Deep Neural Networks (DNN)\n","# and Convolutional Neural Networks (CNN) work with image data.\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"ZADnYe_FBTVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ==========================================================\n","# üß† STEP 1 ‚Äî Load and Explore the CIFAR-10 Dataset\n","# ==========================================================\n","# CIFAR-10 contains 60,000 color images (32x32 pixels, 3 channels)\n","# across 10 classes like airplane, car, cat, etc.\n","\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","print(\"Training data shape:\", X_train.shape)\n","print(\"Single image shape:\", X_train[0].shape)\n","print(\"Test data shape:\", X_test.shape)\n","\n","# Class names for CIFAR-10\n","class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n","               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n","\n","# üîç Visualize one image from each class >> Defines names of the 10 classes (to display later).\n","unique_classes = {}\n","for i, label in enumerate(y_train):\n","    if label[0] not in unique_classes:\n","        unique_classes[label[0]] = i\n","    if len(unique_classes) == 10:\n","        break\n","\n","# Finds one example image for each class (for visualization).\n","plt.figure(figsize=(12, 6))\n","for class_idx, img_idx in unique_classes.items():\n","    plt.subplot(2, 5, class_idx + 1)\n","    plt.imshow(X_train[img_idx])\n","    plt.title(class_names[class_idx])\n","    plt.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"LQKx1UTLBTTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================================\n","# üß© STEP 2 ‚Äî Deep Neural Network (DNN)\n","# ==========================================================\n","# A DNN expects 1D vectors as input. But images are 3D (W, H, Channels)\n","# So we must FLATTEN each image to 1D.\n","\n","# Flatten each image: (32,32,3) ‚Üí (3072,) (because DNNs only accept flat inputs). || X_train.shape[0] ‚Üí number of samples (e.g. 50000) || -1 ‚Üí \"let NumPy automatically calculate this dimension\"\n","\n","X_train_flat = X_train.reshape(X_train.shape[0], -1)\n","X_test_flat = X_test.reshape(X_test.shape[0], -1)\n","\n","# Normalizes pixel values from [0,255] ‚Üí [0,1] for faster training.\n","X_train_flat = X_train_flat / 255.0\n","X_test_flat = X_test_flat / 255.0\n","\n","# Converts class labels (like 3, 5, 7) into one-hot encoded vectors  (e.g., 3 ‚Üí [0,0,0,1,0,0,0,0,0,0]).\n","\n","y_train_cat = to_categorical(y_train, 10)\n","y_test_cat = to_categorical(y_test, 10)\n","\n","# üèóÔ∏è Build the DNN model\n","model_dnn = Sequential([\n","\n","    Dense(512, activation='relu', input_shape=(X_train_flat.shape[1],)),\n","    Dropout(0.5),\n","    Dense(256, activation='relu'),\n","    Dropout(0.5),\n","    Dense(128, activation='relu'),\n","    Dense(10, activation='softmax')\n","])\n","\n","model_dnn.summary()\n","\n","# Compile and train\n","model_dnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","history_dnn = model_dnn.fit(X_train_flat, y_train_cat, epochs=10, batch_size=128,\n","                            validation_data=(X_test_flat, y_test_cat))\n","\n","# Evaluate DNN performance\n","print(\"\\nüìä DNN Evaluation:\")\n","model_dnn.evaluate(X_test_flat, y_test_cat)"],"metadata":{"id":"Vf5fKzy_BTQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# ==========================================================\n","#  STEP 3 ‚Äî Convolutional Neural Network (CNN)\n","# ==========================================================\n","# Unlike DNNs, CNNs can directly handle 2D/3D image data.\n","# They learn spatial features (edges, shapes, textures) automatically.\n","\n","# Reload and normalize image data  >> 0-255   0-1\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n","\n","# üèóÔ∏è Build CNN model\n","model_cnn = Sequential([\n","    # Extract local patterns using filters\n","    Conv2D(64, (3, 3), activation='relu', input_shape=X_train[0].shape),\n","    MaxPooling2D((2, 2)),\n","\n","    Conv2D(32, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","\n","    Conv2D(32, (3, 3), activation='relu'),\n","\n","    # Flatten 3D feature maps ‚Üí 1D vector for dense layers\n","    Flatten(),\n","\n","    # Fully connected layers\n","    Dense(512, activation='relu'),\n","    Dense(256, activation='relu'),\n","\n","    # Output layer (10 classes)\n","    Dense(10, activation='softmax')\n","])\n","\n","model_cnn.summary()\n","\n","# Compile and train CNN\n","model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","history_cnn = model_cnn.fit(X_train, y_train, epochs=5, batch_size=128,\n","                            validation_data=(X_test, y_test))\n","\n","# Evaluate CNN performance\n","print(\"\\nüìä CNN Evaluation:\")\n","model_cnn.evaluate(X_test, y_test)"],"metadata":{"id":"asxELAdmBTN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Byx1_2JQxDoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vvdXF4-pxDkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iVyRlnVRxDia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zboLe1d6xDf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ==========================================================\n","# üßÆ STEP 5 ‚Äî CNN on MNIST (Grayscale Example)\n","# ==========================================================\n","from tensorflow.keras.datasets import mnist\n","\n","(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n","\n","# Add channel dimension (since MNIST is grayscale)\n","x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1) / 255.0\n","x_test_mnist = x_test_mnist.reshape(-1, 28, 28, 1) / 255.0\n","\n","# One-hot encode labels\n","y_train_mnist = to_categorical(y_train_mnist, 10)\n","y_test_mnist = to_categorical(y_test_mnist, 10)\n","\n","print(\"MNIST sample shape:\", x_train_mnist[0].shape)\n","\n","# Small CNN for MNIST\n","model_mnist = Sequential([\n","    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n","    MaxPooling2D((2,2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dense(10, activation='softmax')\n","])\n","\n","model_mnist.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model_mnist.fit(x_train_mnist, y_train_mnist, epochs=3, validation_data=(x_test_mnist, y_test_mnist))\n","\n","# ==========================================================\n","# ‚úÖ SUMMARY\n","# ==========================================================\n","# ‚Ä¢ DNN: Works with flat data (not ideal for images)\n","# ‚Ä¢ CNN: Best for image data ‚Äî learns spatial and visual features\n","# ‚Ä¢ Pooling layers reduce size while keeping important info\n","# ‚Ä¢ Flatten connects CNN ‚Üí Dense layers for classification\n"],"metadata":{"id":"h9s-TFipBzsU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://www.pinecone.io/learn/series/image-search/cnn/  cnn visualized\n","# https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/   cnn architecture"],"metadata":{"id":"Gx339MAfBzpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oxaH2kZZBzmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nznSp8jhBTLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vPq7sZ9AgfY"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"CcqZ8nP4A9LU"},"execution_count":null,"outputs":[]}]}